# AdaptiveSurvivors/.env.example
# You need to change GCP_PROJECT_ID and GCS_TEMP_BUCKET_SUFFIX to your GCP project and bucket names.
# (the other values can be safely left as is or reconfigured if you want)

# --- Kafka / Zookeeper Configuration ---
# Zookeeper client port
ZOO_CLIENT_PORT=2181
# Kafka broker ID (must be unique for each broker)
KAFKA_BROKER_ID=1
# Kafka Zookeeper connection string
KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
# Kafka listener host and port. Use the service name for inter-container communication.
KAFKA_LISTENERS=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092
KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:29092
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL


# --- HDFS Configuration ---
# HDFS NameNode HTTP address
CORE_SITE_FS_DEFAULT_NAME=hdfs://namenode:9000

# --- Flink Configuration ---
# Flink JobManager host
FLINK_JOBMANAGER_HOST=jobmanager

# --- Kafka Connect Configuration ---
# Kafka Connect Bootstrap Servers
CONNECT_BOOTSTRAP_SERVERS=kafka:9092
# Kafka Connect group ID
CONNECT_GROUP_ID=kafka-connect-group
# Kafka Connect worker ID
CONNECT_OFFSET_STORAGE_TOPIC=connect-offsets
CONNECT_CONFIG_STORAGE_TOPIC=connect-configs
CONNECT_STATUS_STORAGE_TOPIC=connect-status
# Kafka Connect plugin path (for HDFS connector)
CONNECT_PLUGIN_PATH=/usr/share/java,/usr/share/confluent-hub-components

# --- Spark Configuration ---
# Spark Master host
SPARK_MASTER_HOST=spark-master
# Spark HDFS NameNode URL
SPARK_HDFS_NAMENODE_URL=hdfs://namenode:9000

# --- BigQuery / GCP Configuration (for Cloud Functions and Spark) ---
#  main.tf needs to match these
# Project ID for GCP project
GCP_PROJECT_ID=your-gcp-project-id
GCP_SPARK_SERVICE_ACCOUNT_NAME=your-service-account-name
# The GCS bucket Spark will use for temporary files when writing to BigQuery (needs to match main.tf)
GCS_TEMP_BUCKET_SUFFIX=your-gcs-temp-bucket-suffix
GEMINI_API_KEY=your-gemini-api-key