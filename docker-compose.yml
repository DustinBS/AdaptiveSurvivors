# /docker-compose.yml

# Orchestrates the local development environment for the Adaptive Survivors backend.
# Sets up Kafka, Zookeeper, HDFS, Flink, and Spark.

services:
  # Zookeeper: Manages metadata for Kafka.
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "${ZOO_CLIENT_PORT}:${ZOO_CLIENT_PORT}"
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/log/zookeeper
    healthcheck:
      test: ["CMD-SHELL", "echo stat | nc localhost ${ZOO_CLIENT_PORT}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka: The event streaming platform.
  kafka:
    image: confluentinc/cp-kafka:7.0.1
    hostname: kafka
    container_name: kafka
    ports:
      - "29092:29092" # External client port
      - "9092:9092"   # Internal Docker network port
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
      KAFKA_LISTENERS: ${KAFKA_LISTENERS}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
      KAFKA_INTER_BROKER_LISTENER_NAME: ${KAFKA_INTER_BROKER_LISTENER_NAME}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
      - kafka_logs:/var/log/kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # HDFS Namenode: Manages the HDFS file system namespace.
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    hostname: namenode
    container_name: namenode
    ports:
      - "9870:9870" # Web UI
    environment:
      - CLUSTER_NAME=test-cluster
      # The bde2020 image uses CORE_CONF_fs_defaultFS to generate the core-site.xml file.
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=1
    env_file:
      - ./.env
    volumes:
      - ./data/namenode:/hadoop/dfs/name
      - ./logs/namenode:/opt/hadoop/logs
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 30s
      timeout: 10s
      retries: 3

  # HDFS Datanode: Stores the actual data blocks in HDFS.
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    hostname: datanode
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./.env
    volumes:
      - ./data/datanode:/hadoop/dfs/data
      - ./logs/datanode:/opt/hadoop/logs
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Flink JobManager: Coordinates Flink jobs.
  jobmanager:
    image: flink:1.14.0-scala_2.12
    hostname: jobmanager
    container_name: jobmanager
    ports:
      - "8082:8081" # Flink Web UI
    command: jobmanager
    environment:
      FLINK_PROPERTIES_FILE: /opt/flink/conf/flink-conf.yaml
      FLINK_REST_ADDRESS: 0.0.0.0
      FLINK_BIND_HOST: 0.0.0.0
      JOB_MANAGER_RPC_ADDRESS: jobmanager
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      HDFS_FEATURE_CACHE_PATH: ${HDFS_FEATURE_CACHE_PATH}
    env_file:
      - ./.env
    volumes:
      - ./logs/flink:/opt/flink/log
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Flink TaskManager: Executes Flink tasks.
  taskmanager:
    image: flink:1.14.0-scala_2.12
    hostname: taskmanager
    container_name: taskmanager
    command: taskmanager
    environment:
      FLINK_TASK_MANAGER_NUMBER_OF_TASK_SLOTS: 1
      FLINK_PROPERTIES: "jobmanager.rpc.address: jobmanager"
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      HDFS_FEATURE_CACHE_PATH: ${HDFS_FEATURE_CACHE_PATH}
    env_file:
      - ./.env
    volumes:
      - ./logs/flink:/opt/flink/log
    depends_on:
      jobmanager:
        condition: service_healthy

  # Kafka Connect: Ingests data to/from Kafka using various connectors.
  kafka-connect:
    build:
      context: ./Backend/KafkaConnect
    hostname: kafka-connect
    container_name: kafka-connect
    ports:
      - "8083:8083" # REST API
    environment:
      CONNECT_BOOTSTRAP_SERVERS: ${CONNECT_BOOTSTRAP_SERVERS}
      CONNECT_GROUP_ID: ${CONNECT_GROUP_ID}
      CONNECT_CONFIG_STORAGE_TOPIC: ${CONNECT_CONFIG_STORAGE_TOPIC}
      CONNECT_OFFSET_STORAGE_TOPIC: ${CONNECT_OFFSET_STORAGE_TOPIC}
      CONNECT_STATUS_STORAGE_TOPIC: ${CONNECT_STATUS_STORAGE_TOPIC}
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=WARN
      CONNECT_PLUGIN_PATH: ${CONNECT_PLUGIN_PATH}
    volumes:
      - ./logs/kafka-connect:/var/log/kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/connectors || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Spark Master: Manages Spark applications.
  spark-master:
    image: bitnami/spark:3.2.0
    hostname: spark-master
    container_name: spark-master
    ports:
      - "8080:8080" # Web UI
      - "7077:7077" # RPC port
    command: "/opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_MASTER_PORT: 7077
      HADOOP_CONF_DIR: /opt/bitnami/spark/conf
      GOOGLE_APPLICATION_CREDENTIALS: /opt/gcp/service-account-key.json
      HDFS_FEATURE_CACHE_PATH: ${HDFS_FEATURE_CACHE_PATH}
      HDFS_DATA_LAKE_PATH: ${HDFS_DATA_LAKE_PATH}
      GCP_PROJECT_ID: ${GCP_PROJECT_ID}
      SPARK_BQ_DATASET: ${SPARK_BQ_DATASET}
      GCS_TEMP_BUCKET: ${GCS_TEMP_BUCKET}
      SPARK_PUBLIC_DNS: localhost
    env_file:
      - ./.env
    volumes:
      - ./logs/spark:/opt/bitnami/spark/logs
      - ./gcp-credentials/service-account-key.json:/opt/gcp/service-account-key.json:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Spark Worker: Executes tasks from the Spark Master.
  spark-worker:
    image: bitnami/spark:3.2.0
    hostname: spark-worker
    container_name: spark-worker
    ports:
      - "8081:8081" # Expose the Spark Worker Web UI port
    command: "/opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://${SPARK_MASTER_HOST}:7077"
    environment:
      SPARK_MASTER_HOST: spark-master
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://${SPARK_MASTER_HOST}:7077
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 2G
      HADOOP_CONF_DIR: /opt/bitnami/spark/conf
      GOOGLE_APPLICATION_CREDENTIALS: /opt/gcp/service-account-key.json
      SPARK_PUBLIC_DNS: localhost
    env_file:
      - ./.env
    volumes:
      - ./logs/spark:/opt/bitnami/spark/logs
      - ./gcp-credentials/service-account-key.json:/opt/gcp/service-account-key.json:ro
    depends_on:
      spark-master:
        condition: service_healthy

  seer-orchestrator:
    build:
      context: ./Backend/PythonWorkers/seer_orchestrator
    container_name: seer-orchestrator
    hostname: seer-orchestrator
    ports:
      - "5001:5001"
    environment:
      - KAFKA_BROKERS=kafka:9092
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/gcp/service-account-key.json
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./gcp-credentials/service-account-key.json:/gcp/service-account-key.json:ro
      - ./Backend/PythonWorkers/seer_orchestrator/config.yml:/app/config.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5001/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5

# Define named volumes for easy cleanup with docker compose down -v.
volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  kafka_logs: